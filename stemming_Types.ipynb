{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming Types\n",
    "we  use in classfication problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eat\",\"eats\",\"eating\",\"eaten\",\"sleeping\",\"writting\",\"going\",\"finally\",\"doing\",\"stepping\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PorterStremmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat------->eat\n",
      "eats------->eat\n",
      "eating------->eat\n",
      "eaten------->eaten\n",
      "sleeping------->sleep\n",
      "writting------->writ\n",
      "going------->go\n",
      "finally------->final\n",
      "doing------->do\n",
      "stepping------->step\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "for word in words:\n",
    "    print(word+\"------->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"congratulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegexStemmer\n",
    "\n",
    "it takes a single regular expression and removes any prefix postfix |suffix  that matches the expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regex=RegexpStemmer('ing$|s$|e$|able$',min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex.stem('eating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowball Stemmmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat------>eat\n",
      "eats------>eat\n",
      "eating------>eat\n",
      "eaten------>eaten\n",
      "sleeping------>sleep\n",
      "writting------>writ\n",
      "going------>go\n",
      "finally------>final\n",
      "doing------>do\n",
      "stepping------>step\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sn_stem=SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(word+\"------>\"+sn_stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chang'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LancasterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "l=LancasterStemmer()\n",
    "l.stem(\"changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet Lemmatizer \n",
    "\n",
    "\n",
    "lemmatization tech is like stemming . The Output will get after lemmmatization is called \"lemma\" which is a root word rather than the root stemm ,the output after stemming ... \n",
    "After lemmatization , we will be getting a valid word that means the same thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# wl=WordNetLemmatizer()\n",
    "# wl.lemmatize(\"hello\")\n",
    "# wl.lemmatize(\"mice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# '''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# pos : Noun - n\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#       Verb - v\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#       adjective - a\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#       adverb - r\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# '''\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mwl\u001b[49m\u001b[38;5;241m.\u001b[39mlemmatize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoing\u001b[39m\u001b[38;5;124m\"\u001b[39m,pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wl' is not defined"
     ]
    }
   ],
   "source": [
    "# '''\n",
    "# pos : Noun - n\n",
    "#       Verb - v\n",
    "#       adjective - a\n",
    "#       adverb - r\n",
    "# '''\n",
    "wl.lemmatize(\"going\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat---->eat\n",
      "eats---->eat\n",
      "eating---->eat\n",
      "eaten---->eat\n",
      "sleeping---->sleep\n",
      "writting---->writting\n",
      "going---->go\n",
      "finally---->finally\n",
      "doing---->do\n",
      "stepping---->step\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    # print(word+\"---->\"+wl.lemmatize(word,pos=\"n\"))\n",
    "    print(word+\"---->\"+wl.lemmatize(word,pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
